{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66471b8b",
   "metadata": {},
   "source": [
    "### Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"YOUR_BUCKET\"\n",
    "# Change the bucket name with your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c5a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r /home/neuro/codes/questionnaire_brain/requirements_multivariate_prediction.txt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d091ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "# Required for regular expression matching\n",
    "import re  \n",
    "# Reqiured for removing files once they are stored in s3 bucket\n",
    "import shutil \n",
    "from glmsingle.glmsingle import GLM_single\n",
    "# Prevent the usage of global variables\n",
    "from noglobal import NoGlobal\n",
    "noglobal = NoGlobal(globals()).noglobal\n",
    "# Prevent warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f42195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the Python version and saves package requirements\n",
    "print(sys.version)\n",
    "!pip freeze > requirements_my_glmsingle_social_isolation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf41055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we decide your working directory\n",
    "initial_directory = \"/home/neuro/mount/\"\n",
    "data_directory = os.path.join(initial_directory, \"run_glmsingle_social_isolation_data/\")\n",
    "# Create data_directory if it doesn't exist\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "print(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace51591",
   "metadata": {},
   "source": [
    "### Set up AWS s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc055e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures AWS directories and copies credentials.\n",
    "if not os.path.exists('/root/.aws'):\n",
    "    os.makedirs('/root/.aws')\n",
    "    # Put credential on host into guest\n",
    "    ! cp /home/neuro/credential/your_s3_credentials ~/.aws/credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and upload files to/from AWS S3.\n",
    "@noglobal\n",
    "def cp_s3(BUCKET_NAME, FROM_PATH, TO_PATH, FILE_NAME, download_or_upload):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(BUCKET_NAME)\n",
    "\n",
    "    from_path = os.path.join(FROM_PATH, FILE_NAME)\n",
    "    to_path = os.path.join(TO_PATH, FILE_NAME)\n",
    "    \n",
    "    if download_or_upload == \"download\":\n",
    "        bucket.download_file(from_path, to_path)\n",
    "        print(\"Downloading from s3\")\n",
    "    elif download_or_upload == \"upload\":\n",
    "        bucket.upload_file(from_path, to_path)\n",
    "        print(\"Uploading to s3\")\n",
    "    else:\n",
    "        raise ValueError('Please specify download or upload.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68241aaf",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db693e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RepetitionTime (preferrably do this only for the first time and than erase them with #)\n",
    "session_id = \"sub-SAXSISO01b\"\n",
    "filename = \"/ds003242-fmriprep/\" + session_id + \"/func/\" + session_id + \"_task-CIC_run-1_space-MNI152NLin6Asym_desc-smoothAROMAnonaggr_bold.json\"\n",
    "s3link = \"s3://openneuro-derivatives/fmriprep\" + filename\n",
    "tr_download_dir = os.path.join(data_directory,\"TR\", \"tr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9206c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --no-sign-request $s3link $tr_download_dir\n",
    "\n",
    "json_open = open(tr_download_dir, 'r')\n",
    "json_load = json.load(json_open)\n",
    "RepetitionTime = json_load['RepetitionTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4083e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads neuroimaging data from the same session from OpenNeuro\n",
    "# Note that some session_id have missing files.\n",
    "# To accommodate this, we first check for runs that have both bold and event file and download only those to avoid error.\n",
    "def download_openneuro_func(session_id):\n",
    "    # Initialize AWS S3 client\n",
    "    s3 = boto3.client('s3', config= boto3.session.Config(signature_version=botocore.UNSIGNED))\n",
    "\n",
    "    # List of available files in the dataset\n",
    "    bold_bucket_name = 'openneuro-derivatives'\n",
    "    event_bucket_name = 'openneuro.org'\n",
    "    bold_prefix = f'fmriprep/ds003242-fmriprep/{session_id}/func/'\n",
    "    event_prefix = f'ds003242/{session_id}/func/'\n",
    "\n",
    "    data_files = []\n",
    "    event_files = []\n",
    "    \n",
    "\n",
    "    # Fetch the list of available bold and event files from each S3 bucket\n",
    "    available_bold_files = s3.list_objects(Bucket=bold_bucket_name, Prefix=bold_prefix)\n",
    "    available_event_files = s3.list_objects(Bucket=event_bucket_name, Prefix=event_prefix)\n",
    "    \n",
    "    \n",
    "    for file_info in available_bold_files.get('Contents', []):\n",
    "        file_name = file_info['Key']\n",
    "        \n",
    "        # Remove 'fmriprep/ds003242-fmriprep/' from the local download directory to set up directory to save bold and event files\n",
    "        # local_file_name = session_id/func/filename.bold.nii.gz\n",
    "        local_file_name = file_name.replace('fmriprep/ds003242-fmriprep/', '')\n",
    "        download_dir = os.path.join(data_directory,local_file_name)\n",
    "\n",
    "        # Check if the file matches the pattern for data files\n",
    "        if \"_space-MNI152NLin6Asym_desc-smoothAROMAnonaggr_bold.nii.gz\" in file_name and \"task-CIC\" in file_name:\n",
    "            \n",
    "            print(f\"Download_dir: {download_dir}\")\n",
    "    \n",
    "            s3link = f\"s3://{bold_bucket_name}/{file_name}\"\n",
    "            run_number = re.search(r\"run-(\\d+)_\", file_name).group(1)\n",
    "    \n",
    "            print(f\"Downloading data file of {session_id} run {run_number}\")\n",
    "            if not os.path.exists(download_dir):\n",
    "                !aws s3 cp --no-sign-request $s3link $download_dir\n",
    "            else:\n",
    "                print(f\"Data file of {session_id} run {run_number} already exists\")\n",
    "            data_files.append(download_dir)\n",
    "\n",
    "            # Create the corresponding event file name\n",
    "            event_file_name = f\"ds003242/{session_id}/func/{session_id}_task-CIC_run-00{run_number}_events.tsv\"\n",
    "            event_s3link = f\"s3://{event_bucket_name}/{event_file_name}\"\n",
    "            event_download_dir = os.path.join(data_directory, session_id, \"func\", f\"{session_id}_task-CIC_run-00{run_number}_events.tsv\")\n",
    "            \n",
    "            print(f\"Event_downlaod_dir: {event_download_dir}\")\n",
    "            \n",
    "            try:\n",
    "                print(f\"Downloading event file of {session_id} run {run_number}\")\n",
    "                !aws s3 cp --no-sign-request $event_s3link $event_download_dir\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Event file {event_file_name} does not exist\")\n",
    "        \n",
    "            event_files.append(event_download_dir)\n",
    "\n",
    "    print(\"Downloading all done!\")\n",
    "    return data_files, event_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664f5c7",
   "metadata": {},
   "source": [
    "### Define functions to conduct GLMsingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6d1aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performs Generalized Linear Model analysis using the downloaded data by GLMsingle.\n",
    "def glmsingle_func(in_files, eventfiles, RepetitionTime):\n",
    "    # get affine transformation matrices for all bold files\n",
    "    print(\"Getting affine data...\")\n",
    "    affine_list = []\n",
    "    for num in range(len(in_files)):\n",
    "        nii1=nib.load(in_files[num])\n",
    "        affine = nii1.header.get_best_affine()\n",
    "        affine_list.append(affine)\n",
    "    \n",
    "    print(\"Comparing affine data across runs...\")    \n",
    "    # If the contents of the affine_list differ between runs, throw an error.\n",
    "    for num in range(len(affine_list)-1):\n",
    "        if not np.allclose(affine_list[num], affine_list[num+1]):\n",
    "            raise ValueError('the contents of the affine_list differ between runs')\n",
    "    \n",
    "    print(\"Loading nibabel images to a list...\")\n",
    "    # Load nibabel images to a python list\n",
    "    data = []\n",
    "    for d in range(len(in_files)):\n",
    "        tmp = nib.load(in_files[d])\n",
    "        data.append(tmp.get_fdata())\n",
    "        print(f\"data shape: {tmp.shape}\")\n",
    "    print(f\"data number: {len(data)}\")\n",
    "\n",
    "    tr          = RepetitionTime\n",
    "    stimdur     = 18\n",
    "    \n",
    "    # Set design\n",
    "    print(\"Setting design...\")\n",
    "    design = []\n",
    "\n",
    "    # Because GLMsingle do not allow to set a varying degree of stimdur for different trial types, we do not include events related to \"Rating\"- it has different stimdur=5\n",
    "    conds = np.array(['Food', 'Social', 'Control'])\n",
    "\n",
    "    nconds = conds.shape[0]\n",
    "\n",
    "    run_events_list = []\n",
    "    for e in (range(len(eventfiles))):\n",
    "        run_events = pd.read_csv(eventfiles[e],delimiter = '\\t')\n",
    "\n",
    "        # remove _1, _2, _3\n",
    "        run_events['trial_type'] = run_events['trial_type'].str[:-2]\n",
    "\n",
    "        # sec to volume\n",
    "        run_events[\"onset_tr\"] = run_events[\"onset\"]/tr\n",
    "        run_events[\"onset_volume\"] = run_events[\"onset_tr\"].round().astype(int)\n",
    "        run_events = run_events.reset_index(drop=True)\n",
    "        run_events_list.append(run_events)\n",
    "\n",
    "        run_design = np.zeros((np.shape(data[e])[3], nconds))\n",
    "        \n",
    "        for c, cond in enumerate(conds):\n",
    "                condidx = np.argwhere(run_events['trial_type'].values == cond)\n",
    "                condvols = run_events['onset_volume'].values[condidx]\n",
    "                run_design[condvols, c] = 1\n",
    "        design.append(run_design)\n",
    "        print(f\"run_design shape: {run_design.shape}\")\n",
    "        \n",
    "    print(f\"design number: {len(design)}\")\n",
    "    \n",
    "    # Set outputdir\n",
    "    current_dir = os.getcwd()\n",
    "    outputdir_glmsingle = os.path.join(current_dir, \"GLMestimatesingletrialoutputs\")\n",
    "\n",
    "    # Set opt\n",
    "    opt = dict()\n",
    "    glmsingle_obj = GLM_single(opt)\n",
    "    \n",
    "    print(f'running GLMsingle...')\n",
    "    \n",
    "    if not os.path.exists(outputdir_glmsingle):\n",
    "    \n",
    "        # run GLMsingle\n",
    "        results_glmsingle = glmsingle_obj.fit(\n",
    "           design,\n",
    "           data,\n",
    "           stimdur,\n",
    "           tr,\n",
    "           outputdir=outputdir_glmsingle)\n",
    "    else:\n",
    "        print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_glmsingle}')\n",
    "    \n",
    "        # load existing file outputs if they exist\n",
    "        results_glmsingle['typed'] = np.load(os.path.join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npy'),allow_pickle=True)\n",
    "\n",
    "    beta_img = nib.Nifti1Image(results_glmsingle['typed']['betasmd'], affine_list[0])\n",
    "    beta_img_savepath = os.path.join(os.getcwd(),'beta_img.nii.gz')\n",
    "    nib.save(beta_img, beta_img_savepath)\n",
    "\n",
    "    events_savepath = os.path.join(current_dir,'events.csv')\n",
    "    df_v = pd.concat(run_events_list)\n",
    "    df_v.to_csv(events_savepath)\n",
    "\n",
    "    beta_img = beta_img_savepath\n",
    "    events = events_savepath\n",
    "        \n",
    "    return beta_img, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a5496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to run the analysis for a specific session for bulk analysis later on for all sessions.\n",
    "def run_analysis(session_id):\n",
    "    in_files, eventfiles = download_openneuro_func(session_id)\n",
    "\n",
    "    sub_dir = os.path.join(data_directory, session_id)\n",
    "    # If you're running this in a Jupyter Notebook and it causes error,\n",
    "    # you might want to replace the next line with a cell magic command.\n",
    "    # Otherwise, the os.chdir() function will work in a regular Python script.\n",
    "    # Set your working directory to yourcurrentworkingdirectory/run_glmsingle_social_isolation_data/session_id\n",
    "    os.chdir(sub_dir)\n",
    "    \n",
    "\n",
    "    beta_img, events = glmsingle_func(in_files, eventfiles, RepetitionTime)\n",
    "\n",
    "    # Saves the output files back to AWS S3.\n",
    "    FROM_PATH   = sub_dir\n",
    "    TO_PATH     = os.path.join(\"outputdir/glmsingle_output/\" + session_id)\n",
    "\n",
    "    FILE_NAME   = \"beta_img.nii.gz\"\n",
    "    cp_s3(BUCKET_NAME, FROM_PATH, TO_PATH, FILE_NAME, \"upload\")\n",
    "\n",
    "    FILE_NAME   = \"events.csv\"\n",
    "    cp_s3(BUCKET_NAME, FROM_PATH, TO_PATH, FILE_NAME, \"upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157182ef",
   "metadata": {},
   "source": [
    "### Actually conduct GLMsingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the disk usage before the operations.\n",
    "!df -h -m --total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all session ids and subsets. There are 32 participants with each having three sessions.\n",
    "skip_numbers = {5, 6, 7, 16, 20, 23, 25, 29, 31, 37} # We skip the numbers with missing files.\n",
    "for i in range(1, 43): # 1 to 42 inclusive\n",
    "    if i in skip_numbers:\n",
    "        continue\n",
    "    for subset in ['b', 'f', 's']:\n",
    "        session_id = f\"sub-SAXSISO{str(i).zfill(2)}{subset}\"\n",
    "        run_analysis (session_id)\n",
    "\n",
    "# Finally after conducting GLMsingle for all available data, erase the repetition time file.\n",
    "shutil.rmtree(data_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
